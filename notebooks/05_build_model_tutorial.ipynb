{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94a724e-00a9-4a43-bd5b-5e94d87ca627",
   "metadata": {},
   "source": [
    "# 05. PyTorch Build the Model Tutorial\n",
    "\n",
    "**Goals**: to understand how a neural network is built.\n",
    "\n",
    "**Source**: [PyTorch Docs Build the Neural Network](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "\n",
    "This notebook will provide instructions for building a neural network that classifies images in the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5df899-c0d2-4e41-903f-b1f1f54cf837",
   "metadata": {},
   "source": [
    "## 1. Imports and Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801d49e7-cde5-46d3-baf9-6b1ab231f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch vesrion: 2.9.0+cpu\n",
      "CUDA is available: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a7833fed6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn # provides all the building blocks you need to build neural network\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Basic checking\n",
    "print(f\"PyTroch vesrion: {torch.__version__}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Establish a seed value to ensure reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2f29d-5a19-4e3e-bf0f-349bc6402df1",
   "metadata": {},
   "source": [
    "## 2. Get Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529925be-553f-4be0-8cae-a7ed75b7f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# We want to be able to train our model on accelerator. The main benefit is speed.\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b6492-9e7d-4cdc-8c01-b06f5cf5a19e",
   "metadata": {},
   "source": [
    "## 3. Define the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a50a42-d19f-4b41-8662-ff34687a3c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "SimpleNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "# Every module in PyTorch is a subclass of nn.Module\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # Reshaping into a one-dimensional tensor\n",
    "        # Initializing the neural network layers\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512), # Applies an affine linear transformation to the incoming data\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512), # second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10) # third hidden layer\n",
    "        )\n",
    "    # Every nn.Module subclass implements the operations on input data in the forward method\n",
    "    def forward(self, x): # Do not call model.forward() directly!\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x) # logits — raw ouput values produced by the model before applying any activation function\n",
    "        return logits\n",
    "\n",
    "# Creating an instance of NeuralNetwork, and moving it to the device\n",
    "model = SimpleNN().to(device)\n",
    "print(f\"Model structure: \\n{model}\")\n",
    "\n",
    "# Creating a random tensor 1x28x28 on 'device'\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits) # dim indicates the dimension along which the values must sum to 1\n",
    "y_pred = pred_probab.argmax(1) \n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57468e6c-7f85-4410-a0c0-b15a1efb7cfe",
   "metadata": {},
   "source": [
    "## 4. Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37254e32-a5b8-4912-85cb-a8ab880cd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Image Size: torch.Size([3, 28, 28])\n",
      "\n",
      "Image Size after 'nn.Flatten': torch.Size([3, 784])\n",
      "\n",
      "Image Size after 'nn.Linear': torch.Size([3, 20])\n",
      "\n",
      "Before ReLU: tensor([[-0.2457,  0.6051, -0.5019,  0.2278,  0.1204, -0.0164,  0.0470,  0.0397,\n",
      "          0.1114,  0.3447, -0.0402,  0.0125,  0.3650,  0.0356,  0.5223, -0.3223,\n",
      "          0.4374,  0.1148,  0.1676, -0.2845],\n",
      "        [-0.0496,  0.4623, -0.2226,  0.4055,  0.1283,  0.3487,  0.1018, -0.2026,\n",
      "          0.0830,  0.2022, -0.2077,  0.2010,  0.1024, -0.4571,  0.3354, -0.4658,\n",
      "          0.4317,  0.2509, -0.1077, -0.0292],\n",
      "        [ 0.0359,  0.3771, -0.3532,  0.0949, -0.0421,  0.5677,  0.0684, -0.1937,\n",
      "          0.1316, -0.1935, -0.1906, -0.0190,  0.0708, -0.1198,  0.1613,  0.1432,\n",
      "          0.4196, -0.1257, -0.3270, -0.3142]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.6051, 0.0000, 0.2278, 0.1204, 0.0000, 0.0470, 0.0397, 0.1114,\n",
      "         0.3447, 0.0000, 0.0125, 0.3650, 0.0356, 0.5223, 0.0000, 0.4374, 0.1148,\n",
      "         0.1676, 0.0000],\n",
      "        [0.0000, 0.4623, 0.0000, 0.4055, 0.1283, 0.3487, 0.1018, 0.0000, 0.0830,\n",
      "         0.2022, 0.0000, 0.2010, 0.1024, 0.0000, 0.3354, 0.0000, 0.4317, 0.2509,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0359, 0.3771, 0.0000, 0.0949, 0.0000, 0.5677, 0.0684, 0.0000, 0.1316,\n",
      "         0.0000, 0.0000, 0.0000, 0.0708, 0.0000, 0.1613, 0.1432, 0.4196, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Taking a sample minibatch of 3 images of size 28x28\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(f\"\\nInput Image Size: {input_image.size()}\")\n",
    "\n",
    "# Converting each 2D 28x28 image into a contiguous array of 784 pixel values\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(f\"\\nImage Size after 'nn.Flatten': {flat_image.size()}\")\n",
    "\n",
    "# Applying a linear transformation on the input using its stored weights and biases\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(f\"\\nImage Size after 'nn.Linear': {hidden1.size()}\")\n",
    "\n",
    "print(f\"\\nBefore ReLU: {hidden1}\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"\\nAfter ReLU: {hidden1}\")\n",
    "\n",
    "# Sequential passes data through all modules in the order defined\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "# Normalizing logits to the range [0,1] \n",
    "pred_probab = nn.Softmax(dim=1)(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fc5f6-6a91-47db-8a9c-74a8f50415b3",
   "metadata": {},
   "source": [
    "## 5. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07abb29a-15e0-4d11-836c-7f31018a3e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: SimpleNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : Parameter containing:\n",
      "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
      "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
      "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
      "        ...,\n",
      "        [-0.0084, -0.0058,  0.0228,  ...,  0.0293,  0.0206, -0.0119],\n",
      "        [ 0.0009,  0.0123,  0.0233,  ..., -0.0127, -0.0286,  0.0204],\n",
      "        [-0.0308,  0.0149, -0.0223,  ...,  0.0130, -0.0236, -0.0194]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([-1.5505e-02, -3.2696e-02, -1.3353e-02,  3.5505e-02,  1.5904e-02,\n",
      "         7.8679e-03,  6.5271e-03,  5.7883e-03,  2.8752e-02, -2.1171e-02,\n",
      "         2.8024e-02,  3.5634e-02,  1.5948e-02,  3.5401e-02,  2.7916e-02,\n",
      "         3.1786e-02, -1.2170e-02,  2.0021e-02, -4.7394e-03,  1.6366e-02,\n",
      "        -2.0817e-02, -2.5212e-02,  2.9135e-02, -1.0824e-02, -1.5246e-02,\n",
      "         1.6319e-02,  3.9158e-03, -2.0049e-02,  6.4671e-06,  9.9425e-03,\n",
      "        -4.1954e-03,  1.0660e-02, -3.3061e-02, -2.6910e-02,  3.1366e-02,\n",
      "        -2.7439e-02,  4.0010e-03,  2.2719e-02,  9.0853e-03, -3.2127e-02,\n",
      "        -2.9268e-02, -3.4516e-02,  2.4226e-02,  2.4354e-02, -1.4881e-02,\n",
      "         2.3167e-03, -2.7620e-03,  1.9277e-02, -2.1683e-02,  1.3494e-02,\n",
      "         2.1517e-03,  7.4790e-03, -1.4102e-02,  2.3649e-02, -9.7762e-03,\n",
      "        -3.4344e-04,  1.8781e-02,  1.4212e-02, -3.3358e-02,  1.9335e-02,\n",
      "        -2.4410e-02,  1.6401e-02, -2.7896e-02,  2.8415e-02,  1.5410e-02,\n",
      "        -1.0950e-02,  5.8264e-03, -2.2293e-02,  2.2976e-02, -1.6541e-02,\n",
      "        -1.4533e-02,  2.6649e-02, -3.2514e-02, -1.8757e-02,  3.4506e-02,\n",
      "         3.3169e-02,  1.6838e-02, -3.5476e-02,  3.5795e-03,  8.8015e-03,\n",
      "        -1.2844e-02, -1.9066e-02, -6.4094e-03, -1.8435e-02, -3.6357e-03,\n",
      "         6.6739e-03,  2.8606e-02, -1.7629e-02, -2.6502e-02,  1.9483e-02,\n",
      "         4.0095e-03,  1.9221e-02, -9.8040e-03,  8.5904e-03, -4.8733e-03,\n",
      "        -1.9769e-02, -1.7464e-02,  3.4789e-02,  1.0478e-02, -4.6240e-03,\n",
      "        -3.5326e-02,  2.3007e-02, -7.7292e-03,  2.8573e-02,  1.2210e-02,\n",
      "        -3.2640e-02,  8.3563e-03, -7.3363e-03,  2.9402e-02,  3.9986e-03,\n",
      "        -2.7580e-02,  2.1792e-02, -2.0154e-02,  2.6137e-02,  1.9127e-02,\n",
      "         1.7685e-03, -6.3045e-03, -1.1410e-02,  4.2802e-03,  8.7162e-03,\n",
      "        -1.0328e-03,  1.4195e-02, -7.4580e-04,  1.0603e-02,  2.2951e-02,\n",
      "        -3.2547e-02,  2.6819e-02, -2.2626e-02,  9.1681e-03,  1.4791e-02,\n",
      "         2.0447e-02,  2.8276e-02,  1.0296e-02, -1.6805e-02, -3.3863e-02,\n",
      "         1.2999e-02, -1.3032e-02, -1.2428e-04, -8.7676e-03, -1.3659e-02,\n",
      "        -4.6620e-03,  2.5752e-02,  2.6885e-02,  3.2684e-02, -4.7462e-03,\n",
      "        -1.4423e-02, -1.0192e-02,  5.7878e-04,  1.3095e-02, -5.5894e-03,\n",
      "        -3.5046e-02, -2.8966e-02,  1.5803e-02,  4.9204e-03, -2.4886e-02,\n",
      "        -3.1402e-02, -2.2429e-02,  1.9891e-03,  7.1186e-03, -3.2151e-02,\n",
      "         1.3356e-02,  2.2203e-02, -4.4811e-03,  1.5752e-03,  2.5082e-03,\n",
      "        -2.4918e-02, -1.3054e-02, -2.3100e-02, -9.7686e-03, -4.3525e-03,\n",
      "         2.1804e-02,  1.3528e-03,  3.7421e-03,  1.2292e-02,  3.5359e-02,\n",
      "        -2.4653e-02,  1.5486e-02, -3.2458e-02,  8.7695e-03, -1.8266e-02,\n",
      "         5.4959e-03,  2.1008e-03, -2.1536e-02, -2.0877e-02, -3.0507e-02,\n",
      "        -2.2892e-02,  2.7179e-02,  1.6648e-02, -1.5959e-02,  3.1663e-02,\n",
      "        -7.6290e-03, -1.9710e-02, -1.2842e-03, -2.6548e-02, -1.9117e-02,\n",
      "         2.0996e-02,  2.2466e-02,  1.1123e-02, -3.6586e-03,  2.5982e-02,\n",
      "         2.4157e-02,  7.9433e-03, -7.7940e-03, -2.9379e-02,  8.4519e-05,\n",
      "        -2.3169e-02,  2.8988e-03, -2.8928e-03,  1.2819e-02, -2.1941e-02,\n",
      "        -3.1103e-02, -2.7007e-02, -9.1788e-03,  2.5283e-04, -2.8113e-02,\n",
      "        -1.9097e-02, -1.9919e-02, -6.0975e-03, -1.0281e-02,  1.6021e-02,\n",
      "         3.0377e-02,  2.7521e-02, -3.5592e-02,  1.4570e-04, -1.6761e-02,\n",
      "         4.5608e-03,  1.4424e-02,  2.0208e-02, -4.3415e-03, -3.0680e-02,\n",
      "         4.6411e-03,  2.4760e-02,  4.2689e-04, -1.8499e-02, -1.4899e-02,\n",
      "         2.0253e-02,  9.3945e-03,  3.1003e-02,  2.5508e-02,  1.9956e-02,\n",
      "        -2.8577e-02,  3.1953e-02,  3.5797e-03, -3.2874e-02,  1.7024e-03,\n",
      "        -2.7793e-02,  2.4403e-03, -2.7997e-02, -3.5454e-02,  2.9492e-02,\n",
      "        -2.1271e-02,  1.3797e-02,  4.4608e-03, -1.3486e-02,  3.7017e-03,\n",
      "        -4.3776e-03, -3.1987e-02, -1.8110e-02, -2.5663e-02, -1.5858e-02,\n",
      "        -1.0639e-02, -1.7287e-02,  1.7442e-02,  3.2514e-02, -1.0000e-02,\n",
      "        -7.7610e-03,  2.4721e-02,  2.4985e-02,  1.2512e-02, -3.3239e-02,\n",
      "         3.4703e-02, -2.5872e-02, -1.1060e-02, -2.0939e-02,  3.2761e-02,\n",
      "         3.1849e-02,  2.4545e-02,  3.2242e-02, -7.4642e-03,  1.6229e-02,\n",
      "         1.3072e-02,  1.5425e-03,  1.8526e-02, -3.5193e-02,  3.1960e-02,\n",
      "         2.0537e-02,  2.3935e-02, -6.8562e-03,  1.0889e-02,  2.8016e-02,\n",
      "         1.0495e-02,  3.4441e-02,  2.3769e-03, -2.6528e-03, -8.5641e-04,\n",
      "         8.4051e-03,  3.5390e-02, -9.4606e-04, -2.4823e-02,  1.7133e-02,\n",
      "         1.0073e-02, -1.7378e-02,  1.8149e-02,  1.1704e-02, -4.0518e-03,\n",
      "         3.5144e-02,  7.6883e-03, -1.7782e-02,  4.5163e-03, -8.8168e-03,\n",
      "        -3.1513e-02, -5.7906e-03,  2.6616e-02,  2.2995e-02, -9.6894e-03,\n",
      "         2.8572e-02, -1.7733e-02, -2.0685e-02,  5.2433e-04, -3.1750e-02,\n",
      "        -2.5244e-02, -2.2610e-02, -8.9112e-03,  4.4882e-03, -3.2259e-03,\n",
      "        -1.4111e-02,  1.1918e-02,  2.6873e-02,  2.9398e-02,  1.0721e-02,\n",
      "        -3.1755e-02,  1.4723e-02, -1.6078e-02,  4.6741e-03, -3.2299e-02,\n",
      "         2.2359e-02, -2.7228e-02,  1.3478e-02, -1.8014e-02,  1.9709e-02,\n",
      "         1.7399e-02, -8.2336e-03, -8.4838e-03,  2.1040e-02,  3.2320e-02,\n",
      "         2.9329e-02,  1.2259e-02,  3.0906e-02,  2.9235e-02,  2.5783e-02,\n",
      "         6.2748e-03, -2.0464e-02,  2.5993e-02, -3.3720e-02, -3.0711e-02,\n",
      "        -1.5984e-02,  9.5294e-03, -2.0663e-02,  5.4320e-03,  5.7096e-03,\n",
      "        -3.2497e-03, -4.0189e-04, -3.4030e-02, -8.2095e-03, -2.0654e-02,\n",
      "         3.1712e-02, -2.7127e-02, -2.4602e-02, -2.6764e-02,  1.5481e-02,\n",
      "         3.0445e-02, -1.2823e-02,  2.5313e-02,  3.0451e-03, -2.7241e-02,\n",
      "         1.2043e-02,  2.4754e-02,  6.0191e-03, -9.9526e-03, -4.8838e-03,\n",
      "         2.1712e-02, -1.6043e-02, -6.8286e-03, -2.1925e-02,  3.0002e-02,\n",
      "        -3.2983e-02,  1.7489e-02,  3.1192e-02, -6.0621e-03,  6.1482e-03,\n",
      "         2.4332e-02,  5.2373e-03,  3.1180e-02,  2.9833e-02,  1.5803e-04,\n",
      "         3.2934e-02, -2.1399e-02, -1.6079e-02, -2.4136e-02, -1.2924e-02,\n",
      "        -2.6332e-02,  2.5571e-02, -5.3862e-03, -2.7214e-02,  2.2297e-02,\n",
      "        -2.8331e-02, -1.8427e-02,  3.5099e-02, -2.4530e-02, -2.5963e-02,\n",
      "         1.6498e-02,  1.9517e-02,  1.2258e-02, -1.5866e-02, -2.5997e-02,\n",
      "        -2.2932e-02,  1.6804e-02, -9.1328e-03,  1.2153e-02, -2.0780e-03,\n",
      "        -1.4573e-02,  3.0545e-02, -2.9639e-02, -6.0484e-03,  1.0352e-02,\n",
      "         1.6022e-02,  3.2838e-02,  1.1885e-02, -2.4253e-02,  3.3081e-02,\n",
      "        -1.9251e-02,  1.1506e-02, -4.8184e-04, -2.7299e-02, -2.0782e-02,\n",
      "         1.3517e-03,  1.7760e-02,  3.5986e-03,  2.4714e-02, -1.0079e-02,\n",
      "         2.5779e-02, -1.5712e-02, -3.4275e-03, -3.2114e-02, -1.9030e-02,\n",
      "        -4.0164e-03,  1.9728e-02, -2.0196e-02, -1.3540e-03,  8.5314e-03,\n",
      "        -2.7475e-03,  1.7043e-02,  9.8973e-03,  3.1675e-02, -3.2689e-02,\n",
      "         3.5536e-02, -3.5905e-03,  8.3893e-03, -2.9810e-02, -6.6934e-03,\n",
      "         2.4526e-02, -1.3814e-02, -2.2872e-02,  6.2209e-03,  1.2617e-02,\n",
      "         2.0895e-02,  9.7385e-03,  1.6024e-02,  3.2260e-02,  2.0915e-02,\n",
      "         3.2568e-02, -3.1779e-02,  1.9668e-02,  7.3523e-04, -3.0771e-02,\n",
      "         2.6365e-02, -3.1076e-03,  3.4686e-02,  1.2093e-02,  2.8295e-02,\n",
      "         1.9963e-02,  1.2238e-02,  1.4661e-02,  1.4490e-02,  1.7613e-02,\n",
      "        -3.0461e-02,  9.2494e-03, -2.6100e-02,  1.9915e-02, -1.9524e-02,\n",
      "         1.2370e-02,  3.0551e-02, -1.0742e-02, -1.6492e-02,  2.0958e-02,\n",
      "         2.8925e-02,  1.8586e-02, -1.7255e-02,  2.6226e-02,  1.6367e-03,\n",
      "        -1.6604e-02,  7.4166e-04,  2.6156e-02,  1.2887e-02,  4.3246e-03,\n",
      "        -1.2390e-02, -4.1765e-03,  9.0212e-03,  4.4437e-03, -2.3132e-02,\n",
      "         2.9016e-02, -2.4390e-02], requires_grad=True)\n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : Parameter containing:\n",
      "tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],\n",
      "        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221],\n",
      "        [-0.0039,  0.0105, -0.0134,  ...,  0.0130, -0.0173, -0.0312],\n",
      "        ...,\n",
      "        [ 0.0377, -0.0211,  0.0249,  ..., -0.0356,  0.0390, -0.0414],\n",
      "        [-0.0441, -0.0406,  0.0108,  ...,  0.0086, -0.0047,  0.0069],\n",
      "        [ 0.0201,  0.0237, -0.0289,  ...,  0.0019,  0.0190, -0.0270]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([ 1.4812e-02, -2.5647e-02,  1.2294e-02,  1.4020e-02, -3.6580e-02,\n",
      "        -2.1421e-02,  3.4543e-02,  1.6798e-03,  3.2846e-02,  2.0313e-02,\n",
      "         3.1581e-02,  1.6131e-02, -2.8228e-02, -2.9073e-02,  1.7684e-02,\n",
      "         4.2598e-02,  2.5351e-02, -3.3256e-02,  2.3423e-02,  2.7261e-02,\n",
      "        -1.5841e-02, -1.3290e-02,  2.3180e-02, -3.6904e-03,  2.6462e-02,\n",
      "         3.9602e-02, -5.1192e-04,  9.0113e-03, -2.4607e-02,  9.1207e-03,\n",
      "        -2.2296e-02,  2.5220e-02, -3.8370e-02, -1.8067e-02,  3.7449e-02,\n",
      "        -2.6822e-02, -4.3288e-02, -4.2587e-02,  1.3593e-02,  3.0193e-02,\n",
      "         2.7782e-03, -3.8336e-03,  5.4861e-03,  2.1876e-02, -2.4915e-02,\n",
      "         3.3789e-02,  2.7401e-02, -3.4106e-02,  3.4674e-02,  7.3363e-03,\n",
      "         2.3367e-02,  2.7529e-02, -3.5972e-04,  1.0914e-02, -3.2901e-02,\n",
      "         4.0513e-02, -1.6840e-02, -2.1052e-02, -4.2338e-02,  2.6447e-02,\n",
      "        -3.9010e-04, -4.2165e-02, -6.7364e-03, -2.7857e-02,  9.8427e-03,\n",
      "        -2.4103e-02, -1.5813e-02, -5.7152e-03, -1.5297e-02, -8.8692e-03,\n",
      "         4.8798e-03, -3.3993e-02,  1.7568e-02,  7.8006e-03, -3.4015e-02,\n",
      "         1.4138e-02,  1.3988e-02, -3.8556e-03, -3.7231e-02,  2.8299e-02,\n",
      "        -2.1103e-02,  4.3876e-02,  1.1625e-02, -1.6815e-02, -3.2218e-02,\n",
      "         5.8935e-03,  7.4378e-03, -2.3856e-02, -1.0152e-02, -3.8660e-03,\n",
      "        -1.2288e-02,  1.4304e-02, -1.1913e-02, -1.5281e-02,  8.0524e-03,\n",
      "         2.7098e-02,  2.1736e-02, -3.6715e-02, -1.8164e-03,  3.3915e-02,\n",
      "        -1.9159e-02, -2.6999e-02, -4.3473e-02,  2.9513e-02,  1.9955e-02,\n",
      "         4.0007e-02, -1.5356e-03, -1.4098e-02, -4.4087e-03,  3.9102e-02,\n",
      "        -1.8014e-02, -8.1230e-03,  3.2635e-02,  2.8817e-03, -3.0834e-02,\n",
      "         1.1929e-03,  2.0838e-03,  5.4504e-03, -4.2352e-02,  3.7451e-02,\n",
      "         2.9010e-02, -3.7543e-02, -1.2400e-02,  3.0988e-02, -8.2679e-03,\n",
      "         4.5486e-03,  4.3883e-02, -4.0593e-02,  2.9196e-02, -2.7894e-02,\n",
      "         3.1899e-02,  8.5088e-03, -3.2941e-02,  1.1152e-02, -4.0936e-02,\n",
      "        -2.8808e-02, -3.6814e-02,  4.2834e-02,  8.7073e-03,  3.4596e-02,\n",
      "         3.4959e-02, -4.0524e-02,  4.0560e-02,  2.7679e-02, -3.7772e-02,\n",
      "        -1.7118e-02, -3.9250e-02,  2.6905e-02, -5.5368e-03,  3.0067e-02,\n",
      "        -1.1878e-02, -2.4272e-02,  9.2770e-03, -3.7626e-02, -1.2292e-02,\n",
      "         4.1672e-02, -8.3022e-04,  2.8397e-02, -4.0262e-02, -3.5077e-02,\n",
      "        -1.3466e-02,  3.4422e-02, -1.0131e-02,  2.8974e-02,  3.2132e-02,\n",
      "         2.3907e-02,  3.2848e-02, -3.4345e-02, -2.0021e-02, -1.0020e-02,\n",
      "         2.2967e-02, -2.6264e-02,  2.7883e-02,  3.9501e-02, -4.4077e-02,\n",
      "         3.7641e-02,  3.8760e-04, -3.3284e-02, -4.1368e-02,  2.5647e-02,\n",
      "        -1.9226e-02,  3.7948e-02, -4.6712e-03,  2.5647e-02, -1.4252e-02,\n",
      "        -1.1064e-02,  1.3566e-02, -4.2900e-05, -1.6602e-02, -2.1823e-02,\n",
      "        -2.6096e-02,  1.0473e-02, -3.5903e-02, -1.9270e-02, -7.9240e-04,\n",
      "        -1.9581e-02, -2.8916e-02,  3.6409e-02, -4.1606e-02, -3.3915e-02,\n",
      "        -8.0962e-03,  3.8926e-02, -2.2555e-02, -2.8791e-02,  2.0411e-02,\n",
      "         8.4925e-03, -3.4079e-02, -7.6020e-03,  4.9038e-04, -1.1851e-02,\n",
      "        -1.7678e-02,  1.2361e-02,  2.3747e-03, -2.5362e-02, -2.1081e-02,\n",
      "        -1.9519e-02,  1.6544e-02, -2.8679e-02, -3.5052e-02,  1.9472e-02,\n",
      "         3.4774e-02,  2.4752e-02, -1.0029e-02, -3.1083e-02,  1.1528e-02,\n",
      "        -7.4867e-03,  1.6678e-02, -3.8176e-02, -1.6052e-02, -2.1188e-02,\n",
      "        -1.9967e-02,  4.3840e-02, -8.6632e-04, -2.0087e-03, -7.5221e-03,\n",
      "        -2.7700e-02,  2.4074e-02,  3.7155e-02,  3.3705e-02,  1.3548e-02,\n",
      "         3.1129e-02,  3.8096e-03,  2.8035e-02, -2.3839e-02, -1.5604e-02,\n",
      "        -4.9602e-03, -4.9681e-06,  4.9418e-03,  2.5215e-02, -1.7327e-02,\n",
      "        -1.8261e-04, -4.0573e-02,  2.6972e-02,  5.1397e-04,  1.9098e-02,\n",
      "         1.7525e-02,  3.4424e-02,  3.9063e-02,  3.7979e-02,  4.7001e-03,\n",
      "         3.9763e-02, -6.9014e-03,  1.6051e-02,  3.4673e-03,  1.9051e-02,\n",
      "        -4.2837e-02,  2.9038e-02,  1.4366e-02, -3.3585e-02,  2.3204e-02,\n",
      "        -4.2288e-02, -4.9044e-03, -3.4123e-02,  3.7048e-02, -1.8170e-02,\n",
      "         2.4809e-02, -4.1618e-02,  2.9392e-02, -1.7527e-02,  4.1949e-02,\n",
      "        -3.4558e-02,  5.1869e-03,  2.3369e-02,  3.5157e-02, -4.1004e-02,\n",
      "         1.5115e-02,  1.7446e-02, -3.0641e-02, -1.0872e-02,  1.9416e-02,\n",
      "         3.2716e-04,  4.9908e-03,  1.3107e-02,  3.2520e-02,  9.4143e-03,\n",
      "         3.8516e-02,  1.0670e-02,  2.6267e-02,  3.3226e-02,  1.2501e-02,\n",
      "         4.4143e-02,  2.5083e-02, -2.0237e-02,  2.2690e-02,  1.1785e-02,\n",
      "         1.8401e-02,  3.7364e-02, -1.5870e-02, -3.6896e-02,  2.5509e-02,\n",
      "        -1.7055e-02, -2.0810e-02, -3.6475e-02, -1.3382e-02,  4.0849e-02,\n",
      "         2.2216e-02,  3.7948e-02,  1.3969e-02, -2.6832e-03,  6.1321e-03,\n",
      "         1.0437e-02, -2.3029e-02, -2.5981e-02,  1.7134e-02, -4.0699e-03,\n",
      "        -1.8737e-02, -2.5277e-02, -3.6840e-02,  3.0274e-02,  3.3121e-02,\n",
      "         8.1835e-04,  3.7375e-02, -1.9928e-02, -1.9131e-02,  2.1863e-02,\n",
      "        -1.0666e-02, -4.4138e-02, -1.5893e-02,  6.0210e-03, -7.8809e-03,\n",
      "         3.0221e-02, -2.5933e-02,  1.5753e-02,  1.9961e-02, -1.6417e-02,\n",
      "         2.0814e-02, -3.1186e-02,  1.5549e-02,  4.1493e-02, -6.6152e-04,\n",
      "        -2.6383e-02,  1.2432e-03, -5.6042e-03,  2.7209e-02, -3.8029e-02,\n",
      "         1.7777e-02,  1.8421e-02,  1.8956e-02, -3.0262e-02,  8.5657e-03,\n",
      "        -2.9231e-02, -2.9483e-02,  1.2652e-02, -6.4160e-03, -2.0478e-02,\n",
      "        -4.8284e-04, -2.4001e-02, -3.0269e-03,  2.1626e-02, -1.1707e-02,\n",
      "         2.4132e-03, -3.0136e-02, -2.2409e-02, -1.2026e-02,  2.5176e-02,\n",
      "         2.5862e-02,  3.3655e-02, -3.9144e-02,  2.8289e-02, -1.4712e-02,\n",
      "         3.2710e-02, -7.9637e-03, -3.9339e-02, -3.9327e-02, -3.9932e-02,\n",
      "        -4.2569e-02,  4.0492e-02,  2.6577e-02,  2.6000e-02,  1.1364e-02,\n",
      "        -1.0378e-02,  3.7112e-02, -2.8771e-02,  1.2320e-02, -3.7099e-04,\n",
      "         3.4174e-03,  3.9009e-02, -2.3200e-03, -2.3957e-02, -4.3486e-02,\n",
      "         1.8341e-02, -2.0584e-02, -3.8209e-02, -4.0137e-02, -1.1246e-02,\n",
      "        -2.6858e-02, -2.0587e-02, -1.6234e-02, -1.4913e-02, -2.1146e-02,\n",
      "         3.0773e-02, -2.7179e-02, -1.5235e-02,  2.8719e-03, -2.8298e-03,\n",
      "         3.7612e-02,  3.9352e-02,  1.2111e-02, -1.6884e-02,  4.9666e-03,\n",
      "         1.5617e-03, -3.1971e-02,  3.5792e-02,  6.0508e-03, -4.2871e-02,\n",
      "        -1.6338e-03, -2.9592e-02, -3.4093e-02,  3.0572e-03, -4.1405e-02,\n",
      "        -3.5591e-02, -2.4735e-02, -1.7026e-02, -3.1961e-02,  1.7965e-02,\n",
      "        -3.3945e-02,  2.8514e-02,  4.1513e-02,  8.2467e-03, -1.1347e-02,\n",
      "         1.8011e-02,  9.1319e-03,  3.7337e-02,  2.7918e-03, -1.9941e-02,\n",
      "        -1.9856e-03,  9.1029e-03,  1.2483e-02,  3.6837e-02,  3.1740e-03,\n",
      "         3.5078e-02, -2.1617e-02, -1.3617e-02,  3.7423e-02,  3.8466e-02,\n",
      "        -3.0887e-02,  4.2135e-02,  2.4516e-02,  3.5817e-02,  3.1384e-02,\n",
      "        -3.3478e-02, -1.5257e-02, -4.3516e-02, -1.8297e-02,  3.3523e-02,\n",
      "         1.7548e-02,  4.2055e-02, -3.9389e-02, -1.1830e-02, -4.4395e-03,\n",
      "         2.0830e-02, -3.7518e-02,  2.8801e-02,  1.8559e-02, -2.4894e-02,\n",
      "         2.6437e-02, -1.7226e-02, -1.0676e-02,  7.2127e-03,  6.1329e-03,\n",
      "         3.4026e-02,  2.7770e-02,  3.9745e-02, -3.4257e-02, -1.1352e-02,\n",
      "         1.6130e-02, -8.8482e-03, -1.7796e-02,  3.6754e-02,  4.3740e-02,\n",
      "        -4.0220e-02, -4.0019e-02, -1.5204e-02,  1.5843e-02, -4.3463e-02,\n",
      "         4.0782e-02,  7.4337e-04, -1.2205e-02, -3.6768e-02,  2.9790e-02,\n",
      "         6.6076e-03,  2.5615e-02,  3.0533e-02,  2.9428e-02,  3.6449e-02,\n",
      "         4.2353e-02,  2.4656e-02, -2.5653e-02,  2.5417e-02, -1.0077e-02,\n",
      "         4.1219e-02, -3.2886e-02], requires_grad=True)\n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : Parameter containing:\n",
      "tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],\n",
      "        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268],\n",
      "        [-0.0424, -0.0095, -0.0132,  ...,  0.0285,  0.0248,  0.0415],\n",
      "        ...,\n",
      "        [-0.0383,  0.0232, -0.0102,  ...,  0.0394,  0.0307,  0.0401],\n",
      "        [-0.0241, -0.0383, -0.0204,  ..., -0.0286,  0.0392, -0.0070],\n",
      "        [-0.0052,  0.0427,  0.0339,  ...,  0.0045, -0.0390, -0.0405]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : Parameter containing:\n",
      "tensor([ 0.0070, -0.0411,  0.0012,  0.0244,  0.0054, -0.0365, -0.0067,  0.0194,\n",
      "         0.0123, -0.0032], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93add47c-0cb9-4440-9394-9ea2d6ac261c",
   "metadata": {},
   "source": [
    "## 6. Conclusion and notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991c420-1951-406c-a590-0b914baae9ea",
   "metadata": {},
   "source": [
    "### We learned:\n",
    "- How to define a basic neural network architecture for classifying FashionMNIST images using PyTorch.\n",
    "- How to use layers from the torch.nn namespace to construct your model.\n",
    "- Every PyTorch model subclasses `nn.Module`, and is typically composed of other modules (layers) in a nested structure.\n",
    "- We organize the network’s architecture by defining layers in `__init__`, and the forward computation in the forward method.\n",
    "- Utilities like `nn.Sequential` can bundle layers for ease of use.\n",
    "### Notes:\n",
    "- For predictions, call the model as a function (do NOT call `forward()` directly).\n",
    "- All parameters (weights, biases) of the model can be viewed using `.parameters()` or `.named_parameters()`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
